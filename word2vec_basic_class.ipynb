{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class word2vec_basic:\n",
    "    '''\n",
    "    \n",
    "    This version has not removed stop words.\n",
    "    Args:\n",
    "    .\n",
    "    word embeddings: self.final_embeddings\n",
    "    words dictionary: self.reversed_dictionary, {0: word_1, 1: word_2 ...}\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, vocabulary_size=50000, read_filename='text8.zip', expected_bytes=31344016, batch_size=128, \n",
    "                 embedding_size=128, skip_window=1, num_skips=2):\n",
    "        # download the dataset\n",
    "        self.url='http://mattmahoney.net/dc/'\n",
    "        self.vocabulary_size=vocabulary_size\n",
    "        self.read_filename=read_filename\n",
    "        self.expected_bytes=expected_bytes\n",
    "        \n",
    "        # construct dataset to be used\n",
    "        self.filename=self.maybe_download()\n",
    "        self.words=self.read_data()\n",
    "        self.data, self.count, self.dictionary, self.reverse_dictionary=self.build_dataset()\n",
    "        self.data_index=0\n",
    "\n",
    "        # model parameters\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size  # Dimension of the embedding vector.\n",
    "        self.skip_window = skip_window       # How many words to consider left and right.\n",
    "        self.num_skips = num_skips         # How many times to reuse an input to generate a label.\n",
    "\n",
    "        # We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "        # validation samples to the words that have a low numeric ID, which by\n",
    "        # construction are also the most frequent.\n",
    "        self.valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "        self.valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "\n",
    "        # replace=False means that we could not choose one for more than one time \n",
    "        self.valid_examples = np.random.choice(self.valid_window, self.valid_size, replace=False)\n",
    "        self.num_sampled = 64    # Number of negative examples to sample.\n",
    "        \n",
    "        # build the graph\n",
    "        self.build_graph()\n",
    "\n",
    "    def maybe_download(self, filename=None, expected_bytes=None):\n",
    "        if (filename is None) or (expected_bytes is None):\n",
    "            filename=self.read_filename\n",
    "            expected_bytes=self.expected_bytes\n",
    "        if not os.path.exists(filename):\n",
    "            filename, _=urllib.request.urlretrieve(url+filename, filename)\n",
    "        stat_info=os.stat(filename)\n",
    "        if not stat_info.st_size==expected_bytes:\n",
    "            print('Wrong size:')\n",
    "            print(stat_info.st_size)\n",
    "            raise Exception('Failed to verify the dataset, could you just download from your browser?')\n",
    "        else:\n",
    "            print('Found and verified', filename)\n",
    "        return filename\n",
    "\n",
    "    # now read the data\n",
    "    def read_data(self, filename=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Extract the first file enclosed in a zip file as a list of words\n",
    "        \"\"\"\n",
    "        if filename is None:\n",
    "            filename=self.filename\n",
    "        with zipfile.ZipFile(filename) as f:\n",
    "            data = f.read(f.namelist()[0]).split()\n",
    "        return data\n",
    "\n",
    "    # Step 2: build the dictionary and replace all those rare words with a UNK token\n",
    "    def build_dataset(self, words=None):\n",
    "        if words is None:\n",
    "            words=self.words\n",
    "        # count: a list of tuples (word_i, frequency_i)\n",
    "        #    later we will replace count['UNK'] with its true frequency\n",
    "        unk_count=0\n",
    "        count=[['UNK', -1]]    # not to use a tuple because tuple is immutable \n",
    "        count.extend(collections.Counter(words).most_common(self.vocabulary_size-1))\n",
    "\n",
    "        # dictionary: keys are the words in count, while values are the index of each words\n",
    "        #     e.g. dictionary={'word_0': 0, 'word_1': 1, ...}\n",
    "        dictionary=dict()\n",
    "        for word, _ in count:\n",
    "            dictionary[word]=len(dictionary)\n",
    "\n",
    "        # data: a list with the same length of the input of this function -- 'words'\n",
    "        #    each element data_i is this: dictionary[word_i in words] \n",
    "        #    -- index of each word in the dictionary\n",
    "        data=list()\n",
    "        for word in words:\n",
    "            if word in dictionary:\n",
    "                data.append(dictionary[word])\n",
    "            else:\n",
    "                data.append(0)\n",
    "                unk_count+=1\n",
    "\n",
    "        #\n",
    "        count[0][1]=unk_count\n",
    "        reverse_dictionary=dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "        return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "    # Step 3: Function to generate a training batch for the skip-gram model.\n",
    "    def generate_batch(self, batch_size=128, num_skips=2, skip_window=1):\n",
    "        #global data_index\n",
    "        assert batch_size % num_skips == 0\n",
    "        assert num_skips <= 2 * skip_window\n",
    "        batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "        labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "        span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "        buffer = collections.deque(maxlen=span)\n",
    "        for _ in range(span):\n",
    "            buffer.append(self.data[self.data_index])\n",
    "            self.data_index = (self.data_index + 1) % len(self.data)\n",
    "        for i in range(batch_size // num_skips):\n",
    "            target = skip_window  # target label at the center of the buffer\n",
    "            targets_to_avoid = [ skip_window ]\n",
    "            for j in range(num_skips):\n",
    "                while target in targets_to_avoid:\n",
    "                    target = random.randint(0, span - 1)\n",
    "                targets_to_avoid.append(target)\n",
    "                batch[i * num_skips + j] = buffer[skip_window]\n",
    "                labels[i * num_skips + j, 0] = buffer[target]\n",
    "            buffer.append(self.data[self.data_index])\n",
    "            self.data_index = (self.data_index + 1) % len(self.data)\n",
    "        return batch, labels\n",
    "    \n",
    "    # build the graph\n",
    "    def build_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "\n",
    "            # Input data.\n",
    "            self.train_inputs = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "            self.train_labels = tf.placeholder(tf.int32, shape=[self.batch_size, 1])\n",
    "            self.valid_dataset = tf.constant(self.valid_examples, dtype=tf.int32)\n",
    "\n",
    "            # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "            with tf.device('/cpu:0'):\n",
    "                # Look up embeddings for inputs.\n",
    "                self.embeddings = tf.Variable(tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0))\n",
    "                self.embed = tf.nn.embedding_lookup(self.embeddings, self.train_inputs)\n",
    "\n",
    "            # Construct the variables for the NCE loss\n",
    "            self.nce_weights = tf.Variable(tf.truncated_normal([self.vocabulary_size, self.embedding_size],\n",
    "                                    stddev=1.0 / math.sqrt(self.embedding_size)))\n",
    "            self.nce_biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "\n",
    "            # Compute the average NCE loss for the batch.\n",
    "            # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "            # time we evaluate the loss.\n",
    "            self.loss = tf.reduce_mean(tf.nn.nce_loss(self.nce_weights, self.nce_biases, self.embed, self.train_labels,\n",
    "                             self.num_sampled, self.vocabulary_size))\n",
    "\n",
    "            # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "            self.optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(self.loss)\n",
    "\n",
    "            # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "            norm = tf.sqrt(tf.reduce_sum(tf.square(self.embeddings), 1, keep_dims=True))\n",
    "            self.normalized_embeddings = self.embeddings / norm\n",
    "            #valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "            #similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "            \n",
    "    # train\n",
    "    def train(self):\n",
    "        num_steps=10001\n",
    "        self.session = tf.InteractiveSession(graph=self.graph)\n",
    "            \n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Initialized')\n",
    "\n",
    "        self.average_loss=0.0\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_labels=self.generate_batch(self.batch_size, self.num_skips, self.skip_window)\n",
    "            feed_dict={self.train_inputs: batch_inputs, self.train_labels: batch_labels}\n",
    "            _, loss_val=self.session.run([self.optimizer, self.loss], feed_dict)\n",
    "            self.average_loss+=loss_val\n",
    "\n",
    "            if step%1000 == 0:\n",
    "                print('average loss at step %s'%step, self.average_loss/step)\n",
    "        self.average_loss/=num_steps\n",
    "        self.final_embeddings=self.normalized_embeddings.eval()\n",
    "            \n",
    "    # visualize the word2vec\n",
    "    def plot_with_labels(self, low_dim_embs, labels, filename='tsne.png'):\n",
    "        assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "        plt.figure(figsize=(18, 18))  #in inches\n",
    "        for i, label in enumerate(labels):\n",
    "            x, y = low_dim_embs[i,:]\n",
    "            plt.scatter(x, y)\n",
    "            plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "        plt.savefig(filename)\n",
    "        \n",
    "    # visualization\n",
    "    def visualize(self, plot_only=50):    \n",
    "        tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "        low_dim_embs = tsne.fit_transform(self.final_embeddings[:plot_only,:])\n",
    "        labels = [self.reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "        self.plot_with_labels(low_dim_embs, labels)\n",
    "       \n",
    "    # close the session\n",
    "    def close(self):\n",
    "        self.session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "m=word2vec_basic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "average loss at step 0 inf\n",
      "average loss at step 1000 145.297402325\n",
      "average loss at step 2000 114.427349751\n",
      "average loss at step 3000 96.1082074471\n",
      "average loss at step 4000 83.5120459063\n",
      "average loss at step 5000 73.8588944111\n",
      "average loss at step 6000 66.7074071606\n",
      "average loss at step 7000 60.6760791579\n",
      "average loss at step 8000 55.8157460918\n",
      "average loss at step 9000 51.7686697176\n",
      "average loss at step 10000 48.2225678111\n"
     ]
    }
   ],
   "source": [
    "m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/matplotlib/collections.py:571: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "m.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.reverse_dictionary[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
